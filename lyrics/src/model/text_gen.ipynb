{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"usable_data.txt\", \"r\", encoding=\"utf-8\")\n",
    "data = file.read()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = [',', '.', '!', '?', ':', ';', '(', ')', '[', ']', '{', '}', '-', '–', '—', '“', '”']\n",
    "# remove stop words/characters\n",
    "data = ''.join([char for char in data if char not in stop_words])\n",
    "\n",
    "# split on space or new line\n",
    "words = data.split(' ') + data.split('\\n')\n",
    "\n",
    "# remove empty strings\n",
    "words = [word for word in words if word != '' and word not in stop_words]\n",
    "\n",
    "# remove duplicates\n",
    "vocab = sorted(set(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size:  77194\n"
     ]
    }
   ],
   "source": [
    "print('vocab size: ', len(vocab))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word to id convertor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_from_words = tf.keras.layers.StringLookup(\n",
    "    vocabulary=list(vocab), mask_token=None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "id to word convertor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_from_ids = tf.keras.layers.StringLookup(\n",
    "    vocabulary=ids_from_words.get_vocabulary(), invert=True, mask_token=None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text from ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_from_ids(ids):\n",
    "  return tf.strings.reduce_join(words_from_ids(ids), axis=-1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(314453,), dtype=int64, numpy=array([ 2015, 30370,  2049, ..., 13260, 33042, 14993], dtype=int64)>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_ids = ids_from_words(words)\n",
    "all_ids"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating ids dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "show ten first words in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "To\n",
      "100\n",
      "\n",
      "\n",
      "199L\n",
      "SPRG\n",
      "Blue\n",
      "Flame\n",
      "Gang\n",
      "Hey\n",
      "petit\n",
      "hey\n",
      "Rentre\n",
      "pas\n"
     ]
    }
   ],
   "source": [
    "for ids in ids_dataset.take(10):\n",
    "    print(words_from_ids(ids).numpy().decode('utf-8'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 100"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'0' b'To' b'100' b'\\n\\n199L\\nSPRG' b'Blue' b'Flame' b'Gang\\nHey'\n",
      " b'petit' b'hey\\nRentre' b'pas' b'dans' b\"l'Rap\" b'le' b'bail' b'est'\n",
      " b'crade' b\"c'est\" b'un' b'panier' b\"d'crabes\\nPas\" b\"l'temps\"\n",
      " b\"d'regarder\" b'les' b'autres' b'bailler\\nMoi' b'quand' b\"j'\\xc3\\xa9cris\"\n",
      " b'le' b'cahier' b'crame\\nTu' b'sais' b'que' b\"j'suis\" b'un' b'des'\n",
      " b'vrais\\nSi' b\"quelqu'un\" b\"s'occupe\" b'bien' b'des' b'frais\\nAvec'\n",
      " b'mon' b'\\xc3\\xa9quipe' b'on' b'atteint' b\"l'Everest\\nC'que\" b'dit'\n",
      " b'Flingue' b'est' b\"vrai\\nJ'voulais\" b'faire' b'un' b'gros' b'son' b'sur'\n",
      " b'mon' b'Blunt' b'Phillies' b'mais\\nIci' b'les' b'chanteurs' b'\\xc3\\xa0'\n",
      " b'la' b'JeanPhilippe' b'Smet' b'finissent' b'ma\\xc3\\xaetre\\nLe' b'fait'\n",
      " b\"qu'ils\" b'fassent' b'encore' b'du' b'biff' b\"m'\\xc3\\xa9gare\" b'la'\n",
      " b'France' b'est\\nAu' b'ralenti' b'comme' b'Keanu' b'Reeves' b'quand'\n",
      " b'il' b'esquive' b'les' b\"balles\\nJ'suis\" b\"l'Neo\" b\"d'cette\" b'matrice'\n",
      " b'crois' b'pas' b'que' b\"j'blague\" b'mamen\\nLa' b'France' b'est'\n",
      " b'conservatrice' b'son' b'Rap' b\"n'\\xc3\\xa9chappe\" b'pas'], shape=(101,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for seq in sequences.take(1):\n",
    "  print(words_from_ids(seq))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define sequence spliting method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(sequence):\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    print(input_text.shape, input_text.dtype)\n",
    "    print(target_text.shape, target_text.dtype)\n",
    "    return input_text, target_text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,) <dtype: 'int64'>\n",
      "(100,) <dtype: 'int64'>\n"
     ]
    }
   ],
   "source": [
    "dataset = sequences.map(split_input_target)\n",
    "# print(type(dataset[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : b\"0To100\\n\\n199L\\nSPRGBlueFlameGang\\nHeypetithey\\nRentrepasdansl'Raplebailestcradec'estunpanierd'crabes\\nPasl'tempsd'regarderlesautresbailler\\nMoiquandj'\\xc3\\xa9crislecahiercrame\\nTusaisquej'suisundesvrais\\nSiquelqu'uns'occupebiendesfrais\\nAvecmon\\xc3\\xa9quipeonatteintl'Everest\\nC'queditFlingueestvrai\\nJ'voulaisfaireungrossonsurmonBluntPhilliesmais\\nIcileschanteurs\\xc3\\xa0laJeanPhilippeSmetfinissentma\\xc3\\xaetre\\nLefaitqu'ilsfassentencoredubiffm'\\xc3\\xa9garelaFranceest\\nAuralenticommeKeanuReevesquandilesquivelesballes\\nJ'suisl'Neod'cettematricecroispasquej'blaguemamen\\nLaFranceestconservatricesonRapn'\\xc3\\xa9chappe\"\n",
      "Target: b\"To100\\n\\n199L\\nSPRGBlueFlameGang\\nHeypetithey\\nRentrepasdansl'Raplebailestcradec'estunpanierd'crabes\\nPasl'tempsd'regarderlesautresbailler\\nMoiquandj'\\xc3\\xa9crislecahiercrame\\nTusaisquej'suisundesvrais\\nSiquelqu'uns'occupebiendesfrais\\nAvecmon\\xc3\\xa9quipeonatteintl'Everest\\nC'queditFlingueestvrai\\nJ'voulaisfaireungrossonsurmonBluntPhilliesmais\\nIcileschanteurs\\xc3\\xa0laJeanPhilippeSmetfinissentma\\xc3\\xaetre\\nLefaitqu'ilsfassentencoredubiffm'\\xc3\\xa9garelaFranceest\\nAuralenticommeKeanuReevesquandilesquivelesballes\\nJ'suisl'Neod'cettematricecroispasquej'blaguemamen\\nLaFranceestconservatricesonRapn'\\xc3\\xa9chappepas\"\n",
      "Input : b'\\xc3\\xa0lar\\xc3\\xa8gle\\nMarianneaprisdusizzurpmaisleschosessepizassent\\nHoesdownG\\'suponapporteleGildas\\nOnapporteleGildas\\nGildasonapporteleGildas\\nDonDadaDonDada\\nToutcequ\\'ilt\\'fautn\\xc3\\xa9gro\\nRetourdesshinobismaiscroispasquec\\'estl\\'fricnotrepilier\\nJ\\'comptepluslesrivauxpli\\xc3\\xa9sencoreimmobiles\\nOnaapprisnotrebizcommeNinoBJackCity\\nFautqu\\'ons\\'lancedansl\\'immobilierfautqu\\'onmultiplienosbilles\\nIlscroientqu\\'lespaillettesetlesstrass\\'vontali\\xc3\\xa9nermessrhabs\\nIlstrouventmesalli\\xc3\\xa9sd\\xc3\\xa9testables\\nIlss\\'disent\"\\xc3\\x87ayestc\\'estdesstars\"\\nOnpasse\\xc3\\xa0basedetassescalientesslashvari\\xc3\\xa9t\\xc3\\xa9s'\n",
      "Target: b'lar\\xc3\\xa8gle\\nMarianneaprisdusizzurpmaisleschosessepizassent\\nHoesdownG\\'suponapporteleGildas\\nOnapporteleGildas\\nGildasonapporteleGildas\\nDonDadaDonDada\\nToutcequ\\'ilt\\'fautn\\xc3\\xa9gro\\nRetourdesshinobismaiscroispasquec\\'estl\\'fricnotrepilier\\nJ\\'comptepluslesrivauxpli\\xc3\\xa9sencoreimmobiles\\nOnaapprisnotrebizcommeNinoBJackCity\\nFautqu\\'ons\\'lancedansl\\'immobilierfautqu\\'onmultiplienosbilles\\nIlscroientqu\\'lespaillettesetlesstrass\\'vontali\\xc3\\xa9nermessrhabs\\nIlstrouventmesalli\\xc3\\xa9sd\\xc3\\xa9testables\\nIlss\\'disent\"\\xc3\\x87ayestc\\'estdesstars\"\\nOnpasse\\xc3\\xa0basedetassescalientesslashvari\\xc3\\xa9t\\xc3\\xa9sd\\'sapes\\nOn'\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in dataset.take(2):\n",
    "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
    "    print(\"Target:\", text_from_ids(target_example).numpy())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create data batch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>,)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = (\n",
    "    dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE),\n",
    ")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, 50, input_length=seq_length),\n",
    "    tf.keras.layers.LSTM(100, return_sequences=True),\n",
    "    tf.keras.layers.LSTM(100),\n",
    "    tf.keras.layers.Dense(100, activation='relu'),\n",
    "    tf.keras.layers.Dense(vocab_size, activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_5 (Embedding)     (None, 100, 50)           3859700   \n",
      "                                                                 \n",
      " lstm_10 (LSTM)              (None, 100, 100)          60400     \n",
      "                                                                 \n",
      " lstm_11 (LSTM)              (None, 100)               80400     \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 77194)             7796594   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11,807,194\n",
      "Trainable params: 11,807,194\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "(None, 100) <dtype: 'float32'>\n",
      "(None, 77194) <dtype: 'float32'>\n",
      "embedding_5 (None, 100) float32\n",
      "lstm_10 (None, 100, 50) float32\n",
      "lstm_11 (None, 100, 100) float32\n",
      "dense_10 (None, 100) float32\n",
      "dense_11 (None, 100) float32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model.summary())\n",
    "[print(i.shape, i.dtype) for i in model.inputs]\n",
    "[print(o.shape, o.dtype) for o in model.outputs]\n",
    "[print(l.name, l.input_shape, l.dtype) for l in model.layers]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Config callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to find data adapter that can handle input: (<class 'tuple'> containing values of types {\"<class 'tensorflow.python.data.ops.dataset_ops.PrefetchDataset'>\"}), <class 'NoneType'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[104], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model\u001b[39m.\u001b[39;49mfit(dataset, batch_size\u001b[39m=\u001b[39;49m\u001b[39m128\u001b[39;49m, epochs\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, callbacks\u001b[39m=\u001b[39;49m[checkpoint_callback])\n",
      "File \u001b[1;32mc:\\Dev\\Projects\\Courses\\IA\\.venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Dev\\Projects\\Courses\\IA\\.venv\\lib\\site-packages\\keras\\engine\\data_adapter.py:1081\u001b[0m, in \u001b[0;36mselect_data_adapter\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m   1078\u001b[0m adapter_cls \u001b[39m=\u001b[39m [\u001b[39mcls\u001b[39m \u001b[39mfor\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39min\u001b[39;00m ALL_ADAPTER_CLS \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mcan_handle(x, y)]\n\u001b[0;32m   1079\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m adapter_cls:\n\u001b[0;32m   1080\u001b[0m     \u001b[39m# TODO(scottzhu): This should be a less implementation-specific error.\u001b[39;00m\n\u001b[1;32m-> 1081\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1082\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFailed to find data adapter that can handle input: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   1083\u001b[0m             _type_name(x), _type_name(y)\n\u001b[0;32m   1084\u001b[0m         )\n\u001b[0;32m   1085\u001b[0m     )\n\u001b[0;32m   1086\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mlen\u001b[39m(adapter_cls) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   1087\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1088\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mData adapters should be mutually exclusive for \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1089\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhandling inputs. Found multiple adapters \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m to handle \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1090\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39minput: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(adapter_cls, _type_name(x), _type_name(y))\n\u001b[0;32m   1091\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to find data adapter that can handle input: (<class 'tuple'> containing values of types {\"<class 'tensorflow.python.data.ops.dataset_ops.PrefetchDataset'>\"}), <class 'NoneType'>"
     ]
    }
   ],
   "source": [
    "model.fit(dataset, batch_size=128, epochs=100, callbacks=[checkpoint_callback])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
