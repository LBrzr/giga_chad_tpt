{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Data collection and pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 100\n",
    "latent_dim = 100\n",
    "num_tokens = 1000\n",
    "max_sequence_length = 20\n",
    "embedding_dim = 64\n",
    "lstm_units = 128\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('usable_data.txt', 'r', encoding='utf-8')\n",
    "text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text.lower()\n",
    "lines = text.split('\\n')\n",
    "line_words = [line.split() for line in lines]\n",
    "\n",
    "vocab = [word for line in line_words for word in line]\n",
    "vocab = sorted(set(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 40460\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=num_tokens, oov_token='<OOV>')\n",
    "# tokenizer = tf_text.WhitespaceTokenizer()\n",
    "tokenizer.fit_on_texts(line_words)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary Size: %d' % vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(line_words)\n",
    "sequences = [seq for seq in sequences if len(seq) <= max_sequence_length]\n",
    "\n",
    "# sequences = np.array(sequences, dtype=np.float64)\n",
    "\n",
    "# _sequences = tokenizer.texts_to_sequences([text])[0]\n",
    "# print(sequences)\n",
    "# print(len(sequences))\n",
    "\n",
    "ids_from_chars = tf.keras.layers.StringLookup(\n",
    "    vocabulary=vocab, mask_token=None)\n",
    "\n",
    "all_ids = ids_from_chars(tf.constant(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
    "dataset = dataset.shuffle(buffer_size=10000)\n",
    "dataset = dataset.batch(batch_size, drop_remainder=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 40458\n",
      "Number of batches: 316\n"
     ]
    }
   ],
   "source": [
    "# Determine the number of sequences in the dataset\n",
    "num_sequences = len(all_ids)\n",
    "# Calculate the number of batches\n",
    "num_batches = num_sequences // batch_size\n",
    "\n",
    "print('Number of sequences:', num_sequences)\n",
    "print('Number of batches:', num_batches)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define generator model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(latent_dim,)),\n",
    "    tf.keras.layers.Dense(embedding_dim),\n",
    "    tf.keras.layers.Dense(max_sequence_length * lstm_units),\n",
    "    tf.keras.layers.Reshape((max_sequence_length, lstm_units)),\n",
    "    tf.keras.layers.LSTM(lstm_units, return_sequences=True),\n",
    "    tf.keras.layers.LSTM(lstm_units, return_sequences=True),\n",
    "    tf.keras.layers.TimeDistributed(\n",
    "        tf.keras.layers.Dense(num_tokens, activation='softmax'))\n",
    "])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define discriminator model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(max_sequence_length, num_tokens)),\n",
    "    tf.keras.layers.LSTM(lstm_units, return_sequences=True),\n",
    "    tf.keras.layers.LSTM(lstm_units),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define loss functions and optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "generator_optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_trainable_variables = generator.trainable_variables\n",
    "disc_trainable_variables = discriminator.trainable_variables\n",
    "\n",
    "generator_optimizer.build(gen_trainable_variables)\n",
    "discriminator_optimizer.build(disc_trainable_variables)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(real_sequences):\n",
    "    # Generate random noise vectors\n",
    "    noise = tf.random.normal([batch_size, latent_dim])\n",
    "\n",
    "    # Generate fake sequences using generator\n",
    "    with tf.GradientTape() as gen_tape:\n",
    "        generated_sequences = generator(noise, training=True)\n",
    "\n",
    "        # Evaluate discriminator on real and fake sequences\n",
    "        real_scores = discriminator(real_sequences, training=True)\n",
    "        fake_scores = discriminator(generated_sequences, training=True)\n",
    "\n",
    "        # Compute generator loss and gradients\n",
    "        gen_loss = cross_entropy(tf.ones_like(fake_scores), fake_scores)\n",
    "        gen_gradients = gen_tape.gradient(\n",
    "            gen_loss, generator.trainable_variables)\n",
    "\n",
    "    # Update generator\n",
    "    generator_optimizer.apply_gradients(\n",
    "        zip(gen_gradients, generator.trainable_variables))\n",
    "\n",
    "    # Train discriminator on real sequences\n",
    "    with tf.GradientTape() as disc_tape:\n",
    "        real_scores = discriminator(real_sequences, training=True)\n",
    "        real_loss = cross_entropy(tf.ones_like(real_scores), real_scores)\n",
    "        real_gradients = disc_tape.gradient(\n",
    "            real_loss, discriminator.trainable_variables)\n",
    "\n",
    "    # Train discriminator on fake sequences\n",
    "    with tf.GradientTape() as disc_tape:\n",
    "        fake_scores = discriminator(generated_sequences, training=True)\n",
    "        fake_loss = cross_entropy(tf.zeros_like(fake_scores), fake_scores)\n",
    "        fake_gradients = disc_tape.gradient(\n",
    "            fake_loss, discriminator.trainable_variables)\n",
    "\n",
    "    # Update discriminator\n",
    "    discriminator_gradients = [real_grad + fake_grad for real_grad,\n",
    "                               fake_grad in zip(real_gradients, fake_gradients)]\n",
    "    discriminator_optimizer.apply_gradients(\n",
    "        zip(discriminator_gradients, discriminator.trainable_variables))\n",
    "    \n",
    "    print(\n",
    "        f\"Generator loss: {gen_loss:.4f} | Discriminator loss: [real_loss: {real_loss:.4f}, fake_loss: {fake_loss:.4f}]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(fake_scores):\n",
    "    return cross_entropy(tf.ones_like(fake_scores), fake_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator(noise, target_batch):\n",
    "    with tf.GradientTape() as gen_tape:\n",
    "        # Generate fake text from noise input\n",
    "        generated_text = generator(noise, training=True)\n",
    "        # Get the discriminator's decision on the generated text\n",
    "        disc_decision = discriminator(generated_text, training=False)\n",
    "        # Calculate generator's loss based on the discriminator's decision\n",
    "        gen_loss = generator_loss(disc_decision)\n",
    "    # Get the generator's trainable variables\n",
    "    gen_variables = generator.trainable_variables\n",
    "    # Calculate the gradients of generator's loss with respect to its variables\n",
    "    gen_gradients = gen_tape.gradient(gen_loss, gen_variables)\n",
    "    # Use optimizer to apply the gradients to the generator's variables\n",
    "    generator_optimizer.apply_gradients(zip(gen_gradients, gen_variables))\n",
    "    return gen_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(real_decision, fake_decision):\n",
    "    real_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "        labels=tf.ones_like(real_decision), logits=real_decision))\n",
    "    fake_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "        labels=tf.zeros_like(fake_decision), logits=fake_decision))\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_discriminator(input_batch, target_batch, noise):\n",
    "    with tf.GradientTape() as disc_tape:\n",
    "        # Generate fake text from noise input using generator\n",
    "        generated_text = generator(noise, training=True)\n",
    "        # Concatenate real input and target text\n",
    "        combined_input = tf.concat([input_batch, target_batch], axis=-1)\n",
    "        # Concatenate fake input and generated text\n",
    "        combined_generated = tf.concat([input_batch, generated_text], axis=-1)\n",
    "        # Get the discriminator's decision on the real and fake inputs\n",
    "        real_decision = discriminator(combined_input, training=True)\n",
    "        fake_decision = discriminator(combined_generated, training=True)\n",
    "        # Calculate discriminator's loss based on real and fake decisions\n",
    "        disc_loss = discriminator_loss(real_decision, fake_decision)\n",
    "    # Get the discriminator's trainable variables\n",
    "    disc_variables = discriminator.trainable_variables\n",
    "    # Calculate the gradients of discriminator's loss with respect to its variables\n",
    "    disc_gradients = disc_tape.gradient(disc_loss, disc_variables)\n",
    "    # Use optimizer to apply the gradients to the discriminator's variables\n",
    "    discriminator_optimizer.apply_gradients(\n",
    "        zip(disc_gradients, disc_variables))\n",
    "    return disc_loss\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Noise: tf.Tensor(\n",
      "[[-4.1729024e-01 -6.2729679e-02  1.0819298e+00 ... -1.7141645e-01\n",
      "  -1.1548320e+00  5.2107024e-01]\n",
      " [ 1.9947000e-01 -1.2159010e+00  4.6054047e-01 ... -1.2564840e+00\n",
      "  -7.5919509e-02 -1.0968996e+00]\n",
      " [-1.3574277e+00 -3.1679302e-01 -2.7606523e-01 ...  2.8225920e-01\n",
      "   1.1259772e+00 -1.6625422e-01]\n",
      " ...\n",
      " [-2.1120114e-03  6.0322400e-02  2.4923999e+00 ... -7.9645211e-01\n",
      "   7.6475954e-01  1.3280016e+00]\n",
      " [-9.5229769e-01  5.2533656e-01 -1.1828583e+00 ...  7.0421040e-01\n",
      "   9.1505182e-01 -1.3617281e+00]\n",
      " [-6.6880065e-01 -2.0706030e-02  2.1592143e+00 ... -1.0033648e+00\n",
      "  -3.5230283e-02 -1.1656549e+00]], shape=(128, 100), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Dev\\Projects\\Courses\\IA\\.venv\\lib\\site-packages\\keras\\backend.py:5703: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot iterate over a scalar tensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 31\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mNoise:\u001b[39m\u001b[39m\"\u001b[39m, noise)\n\u001b[0;32m     26\u001b[0m \u001b[39m# generator.summary()\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \n\u001b[0;32m     28\u001b[0m \n\u001b[0;32m     29\u001b[0m \n\u001b[0;32m     30\u001b[0m \u001b[39m# Train the generator to fool the discriminator\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m gan_loss, gen_loss \u001b[39m=\u001b[39m train_generator(noise, target_batch)\n\u001b[0;32m     33\u001b[0m \u001b[39m# Train the discriminator on real and generated data\u001b[39;00m\n\u001b[0;32m     34\u001b[0m dis_loss \u001b[39m=\u001b[39m train_discriminator(input_batch, target_batch, noise)\n",
      "File \u001b[1;32mc:\\Dev\\Projects\\Courses\\IA\\.venv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:582\u001b[0m, in \u001b[0;36mTensor.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    580\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCannot iterate over a tensor with unknown shape.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    581\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m shape:\n\u001b[1;32m--> 582\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCannot iterate over a scalar tensor.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    583\u001b[0m \u001b[39mif\u001b[39;00m shape[\u001b[39m0\u001b[39m] \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    584\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m    585\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mCannot iterate over a tensor with unknown first dimension.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: Cannot iterate over a scalar tensor."
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    print(\"Epoch:\", epoch+1)\n",
    "    for batch in range(num_batches):\n",
    "        # Get a random batch of input sequences\n",
    "        # idx = np.random.randint(0, num_sequences-batch_size)\n",
    "        # get the input and target batch from dataset\n",
    "        dataset = dataset.shuffle(buffer_size=buffer_size)\n",
    "        input_batch = dataset.batch(batch_size - 1, drop_remainder=True)\n",
    "\n",
    "        # ! input_batch = sequences[idx:idx+batch_size, :-1]\n",
    "        # input_batch = all_ids[idx:idx+batch_size, :-1]\n",
    "        # ! target_batch = np.expand_dims(sequences[-1], axis=-1)\n",
    "        # target_batch = np.expand_dims(all_ids[-1], axis=-1)\n",
    "        target_batch = np.expand_dims(dataset.batch(1, drop_remainder=True), axis=-1)\n",
    "\n",
    "        # Generate noise for the generator\n",
    "        noise = tf.random.normal(shape=(batch_size, latent_dim), dtype=tf.float32).numpy()\n",
    "\n",
    "        # noise = np.random.normal(    0, 1, size=(batch_size, latent_dim))\n",
    "\n",
    "        noise = tf.constant(noise)\n",
    "\n",
    "        # noise = tf.Tensor(noise, noise.shape, noise.dtype)\n",
    "\n",
    "        print(\"Noise:\", noise)\n",
    "        # generator.summary()\n",
    "        \n",
    "\n",
    "\n",
    "        # Train the generator to fool the discriminator\n",
    "        gan_loss, gen_loss = train_generator(noise, target_batch)\n",
    "\n",
    "        # Train the discriminator on real and generated data\n",
    "        dis_loss = train_discriminator(input_batch, target_batch, noise)\n",
    "\n",
    "        print(f\"Generator loss: {gen_loss:.4f} | Discriminator loss: {dis_loss:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
